{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b72b64b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jop Brouwer\\AppData\\Local\\Temp\\ipykernel_20476\\1940578210.py:23: DtypeWarning: Columns (15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_surprise = pd.read_csv('US_economic_releases_events.csv')\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[32m~\\AppData\\Local\\Temp\\ipykernel_20476\\1940578210.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     46\u001b[39m df_surprise.rename(columns={\u001b[33m'Unnamed: 0'\u001b[39m: \u001b[33m'Date'\u001b[39m}, inplace=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     47\u001b[39m df_surprise.reset_index(drop=\u001b[38;5;28;01mTrue\u001b[39;00m, inplace=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     48\u001b[39m \u001b[38;5;66;03m# FIX: Use str.cat() for robust string concatenation to avoid alignment errors\u001b[39;00m\n\u001b[32m     49\u001b[39m df_surprise['DateTime'] = pd.to_datetime(\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m     df_surprise[\u001b[33m'Date'\u001b[39m].astype(str).str.cat(df_surprise[\u001b[33m'Time'\u001b[39m].astype(str), sep=\u001b[33m' '\u001b[39m),\n\u001b[32m     51\u001b[39m     errors=\u001b[33m'coerce'\u001b[39m\n\u001b[32m     52\u001b[39m )\n\u001b[32m     53\u001b[39m \n",
      "\u001b[32mc:\\Users\\Jop Brouwer\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\generic.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   6295\u001b[39m             \u001b[38;5;28;01mand\u001b[39;00m name \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01min\u001b[39;00m self._accessors\n\u001b[32m   6296\u001b[39m             \u001b[38;5;28;01mand\u001b[39;00m self._info_axis._can_hold_identifiers_and_holds_name(name)\n\u001b[32m   6297\u001b[39m         ):\n\u001b[32m   6298\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m self[name]\n\u001b[32m-> \u001b[39m\u001b[32m6299\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m object.__getattribute__(self, name)\n",
      "\u001b[31mAttributeError\u001b[39m: 'DataFrame' object has no attribute 'str'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import pandas_datareader as pdr\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "import xgboost as xgb\n",
    "\n",
    "# --- Configuration ---\n",
    "holding_period = 20\n",
    "transaction_cost = 0.000025\n",
    "total_cost = 2 * transaction_cost\n",
    "\n",
    "# --- Data Loading ---\n",
    "# Load and concatenate E-mini S&P 500 futures data\n",
    "df_es = pd.DataFrame()\n",
    "for i in range(1, 12):\n",
    "    df_temp = pd.read_csv(f'ES_part_{i}.csv')\n",
    "    df_es = pd.concat([df_es, df_temp], ignore_index=True)\n",
    "\n",
    "# Load economic surprise data\n",
    "df_surprise = pd.read_csv('US_economic_releases_events.csv')\n",
    "\n",
    "# --- Data Cleaning and Preprocessing ---\n",
    "df_surprise.drop(columns=['S', 'Month', 'Surv(A)', 'Surv(H)', 'Surv(L)'], inplace=True)\n",
    "df_surprise.drop(columns=['Flag', 'Country/Region', 'Day', 'C', 'Category','Subcategory', 'Std Dev', 'Period', 'Actual'], inplace=True, errors='ignore')\n",
    "\n",
    "\n",
    "df_surprise.replace(\"--\", pd.NA, inplace=True)\n",
    "df_surprise.dropna(subset=['Surprise'], inplace=True)\n",
    "df_surprise = df_surprise[df_surprise['Surprise'] != 0]\n",
    "\n",
    "df_surprise['Surprise'] = pd.to_numeric(df_surprise['Surprise'], errors='coerce')\n",
    "df_surprise.dropna(subset=['Surprise'], inplace=True)\n",
    "df_surprise = df_surprise[df_surprise['Surprise'] != 0]\n",
    "\n",
    "df_surprise.dropna(subset=['Time'], inplace=True)\n",
    "\n",
    "# Winsorizing the 'Surprise' column\n",
    "lower_bound = df_surprise['Surprise'].quantile(0.005)\n",
    "upper_bound = df_surprise['Surprise'].quantile(0.995)\n",
    "df_surprise = df_surprise[(df_surprise['Surprise'] >= lower_bound) & (df_surprise['Surprise'] <= upper_bound)]\n",
    "\n",
    "# --- DateTime Conversion ---\n",
    "df_surprise.rename(columns={'Unnamed: 0': 'Date'}, inplace=True)\n",
    "df_surprise.reset_index(drop=True, inplace=True)\n",
    "# FIX: Use str.cat() for robust string concatenation to avoid alignment errors\n",
    "df_surprise['DateTime'] = pd.to_datetime(\n",
    "    df_surprise['Date'].astype(str).str.cat(df_surprise['Time'].astype(str), sep=' '),\n",
    "    errors='coerce'\n",
    ")\n",
    "\n",
    "# FIX: Use str.cat() for robust string concatenation here as well\n",
    "df_es['DateTime'] = pd.to_datetime(\n",
    "    df_es['Date'].astype(str).str.cat(df_es['Time'].astype(str), sep=' '),\n",
    "    format='%m/%d/%Y %H:%M',\n",
    "    errors='coerce'\n",
    ")\n",
    "df_es['Time'] = df_es['DateTime'].dt.strftime('%H:%M:%S')\n",
    "\n",
    "\n",
    "# --- Merging DataFrames ---\n",
    "df_combined = pd.merge(df_es, df_surprise, on='DateTime', how='outer', suffixes=('_es', '_surprise'))\n",
    "df_combined.dropna(subset=['Open'], inplace=True)\n",
    "df_combined = df_combined.sort_values(by='DateTime').reset_index(drop=True)\n",
    "\n",
    "# --- Feature Engineering ---\n",
    "df_combined['Surprise Occurred'] = df_combined['Surprise'].notna()\n",
    "df_combined['First Post Surprise'] = df_combined['Surprise Occurred'].shift(1).fillna(False)\n",
    "\n",
    "# --- Return and Profit Calculation ---\n",
    "holding_period_half = int(holding_period * 0.5)\n",
    "holding_period_double = int(holding_period * 2)\n",
    "\n",
    "df_combined['Return'] = (df_combined['Open'].shift(-holding_period) - df_combined['Open']) / df_combined['Open']\n",
    "df_combined['Return_half'] = (df_combined['Open'].shift(-holding_period_half) - df_combined['Open']) / df_combined['Open']\n",
    "df_combined['Return_double'] = (df_combined['Open'].shift(-holding_period_double) - df_combined['Open']) / df_combined['Open']\n",
    "\n",
    "for horizon in ['', '_half', '_double']:\n",
    "    df_combined[f'Profit{horizon}'] = np.nan\n",
    "    profitable_long = df_combined[f'Return{horizon}'] > total_cost\n",
    "    profitable_short = -df_combined[f'Return{horizon}'] > total_cost\n",
    "    df_combined.loc[profitable_long, f'Profit{horizon}'] = 1\n",
    "    df_combined.loc[profitable_short, f'Profit{horizon}'] = 2\n",
    "    df_combined[f'Profit{horizon}'].fillna(0, inplace=True)\n",
    "\n",
    "# --- Technical Features ---\n",
    "def create_technical_features(df):\n",
    "    df['Price'] = df['Open']\n",
    "    # Calculate the 'R' (Range) feature\n",
    "    df['R'] = df['High'] - df['Low']\n",
    "    ma_windows = [5, 10, 15, 20, 50, 100, 200]\n",
    "    for window in ma_windows:\n",
    "        df[f'SMA{window}'] = df['Open'].rolling(window).mean()\n",
    "        df[f'SMA{window}Cross'] = (df['Open'] > df[f'SMA{window}']).astype(int)\n",
    "\n",
    "    trend_periods = [10, 15, 50]\n",
    "    for period in trend_periods:\n",
    "        df[f'UpDown{period}'] = np.sign(df['Open'].pct_change(periods=period, fill_method='pad'))\n",
    "    return df\n",
    "\n",
    "df_combined = create_technical_features(df_combined)\n",
    "df_combined['Volume_L1'] = df_combined['Volume'].shift(1)\n",
    "\n",
    "# --- Prepare Data for ML ---\n",
    "df_intm = df_combined[df_combined['First Post Surprise']].copy()\n",
    "df_intm['Surprise'] = df_intm['Surprise'].ffill()\n",
    "df_intm = df_intm.dropna(subset=['Return', 'Return_half', 'Return_double'])\n",
    "\n",
    "ticker_dummies = pd.get_dummies(df_intm['Ticker'], prefix='Ticker')\n",
    "df_ml = pd.concat([df_intm, ticker_dummies], axis=1)\n",
    "\n",
    "ticker_cols = [col for col in df_ml.columns if 'Ticker_' in col]\n",
    "feature_cols = ['Surprise', 'Volume_L1', 'R', 'SMA5', 'SMA10', 'SMA20', 'SMA200',\n",
    "                'SMA5Cross', 'SMA10Cross','SMA15Cross', 'SMA20Cross',\n",
    "                'SMA50Cross', 'SMA100Cross', 'SMA200Cross', 'UpDown10',\n",
    "                'UpDown15', 'UpDown50'] + ticker_cols\n",
    "\n",
    "# --- Macroeconomic Data ---\n",
    "start = datetime.datetime(1997, 1, 1)\n",
    "end = datetime.datetime.today()\n",
    "\n",
    "# Get GDP Growth Data from FRED\n",
    "gdp_gr = pdr.DataReader('A191RL1Q225SBEA', 'fred', start, end)\n",
    "gdp_gr_ml = gdp_gr.reset_index()\n",
    "gdp_gr_ml.rename(columns={'A191RL1Q225SBEA': 'gdp_gr'}, inplace=True)\n",
    "gdp_gr_ml['DateTime'] = pd.to_datetime(gdp_gr_ml['DATE']) + pd.Timedelta(hours=23, minutes=59)\n",
    "gdp_gr_ml = gdp_gr_ml[['DateTime', 'gdp_gr']]\n",
    "\n",
    "# Get VIX Data from FRED\n",
    "vix = pdr.DataReader('VIXCLS', 'fred', start, end)\n",
    "vix_ml = vix.reset_index()\n",
    "vix_ml.rename(columns={'VIXCLS': 'VIX'}, inplace=True)\n",
    "vix_ml['DateTime'] = pd.to_datetime(vix_ml['DATE']) + pd.Timedelta(hours=23, minutes=59)\n",
    "vix_ml = vix_ml[['DateTime', 'VIX']]\n",
    "\n",
    "\n",
    "df_ml = pd.merge_asof(df_ml.sort_values('DateTime'), gdp_gr_ml.sort_values('DateTime'), on='DateTime', direction='backward')\n",
    "df_ml = pd.merge_asof(df_ml.sort_values('DateTime'), vix_ml.sort_values('DateTime'), on='DateTime', direction='backward')\n",
    "df_ml.rename(columns={'gdp_gr': 'last_gdp_gr', 'VIX': 'last_vix'}, inplace=True)\n",
    "feature_cols.extend(['last_gdp_gr', 'last_vix'])\n",
    "\n",
    "df_ml = df_ml.dropna(subset=feature_cols).reset_index(drop=True)\n",
    "\n",
    "# --- XGBoost Model and Backtesting ---\n",
    "min_date = df_ml['DateTime'].min()\n",
    "max_date = df_ml['DateTime'].max()\n",
    "train_end_date = min_date + (max_date - min_date) * 0.3\n",
    "\n",
    "results = []\n",
    "all_test_returns, all_dates, all_test_preds, all_test_truths, all_test_probas, fold_mean_returns, best_horizons_list = [], [], [], [], [], [], []\n",
    "\n",
    "def calculate_grouped_returns(df_group, horizon):\n",
    "    active_trades = df_group[df_group['pred'] != 0]\n",
    "    if active_trades.empty:\n",
    "        return 0.0\n",
    "    longs = active_trades[active_trades['pred'] == 1]\n",
    "    shorts = active_trades[active_trades['pred'] == 2]\n",
    "    total_confidence = longs['proba'].sum() + shorts['proba'].sum()\n",
    "    if total_confidence == 0:\n",
    "        return 0.0\n",
    "    num_trades = len(longs) + len(shorts)\n",
    "    total_costs = total_cost * num_trades\n",
    "    long_returns = (longs[f'Return{horizon}'] * (longs['proba'] / total_confidence)).sum()\n",
    "    short_returns = (-shorts[f'Return{horizon}'] * (shorts['proba'] / total_confidence)).sum()\n",
    "    return (long_returns + short_returns) - total_costs\n",
    "\n",
    "while True:\n",
    "    val_start_date = train_end_date + pd.DateOffset(days=1)\n",
    "    val_end_date = val_start_date + pd.DateOffset(years=1)\n",
    "    test_start_date = val_end_date + pd.DateOffset(days=1)\n",
    "    test_end_date = test_start_date + pd.DateOffset(years=1)\n",
    "\n",
    "    if val_end_date > max_date or test_end_date > max_date:\n",
    "        break\n",
    "\n",
    "    train_data = df_ml[df_ml['DateTime'] <= train_end_date]\n",
    "    val_data = df_ml[(df_ml['DateTime'] >= val_start_date) & (df_ml['DateTime'] <= val_end_date)]\n",
    "    test_data = df_ml[(df_ml['DateTime'] >= test_start_date) & (df_ml['DateTime'] <= test_end_date)]\n",
    "\n",
    "    if val_data.empty or test_data.empty:\n",
    "        train_end_date = val_end_date\n",
    "        continue\n",
    "\n",
    "    horizon_performance = {}\n",
    "    models = {}\n",
    "    for horizon in ['', '_half', '_double']:\n",
    "        X_train = train_data[feature_cols]\n",
    "        y_train = train_data[f'Profit{horizon}']\n",
    "        X_val = val_data[feature_cols]\n",
    "        y_val = val_data[f'Profit{horizon}']\n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "        model = xgb.XGBClassifier(num_class=3, n_estimators=60, max_depth=7, random_state=42, seed=42, use_label_encoder=False, eval_metric='mlogloss')\n",
    "        model.fit(X_train_scaled, y_train, eval_set=[(X_val_scaled, y_val)], verbose=False)\n",
    "        \n",
    "        val_preds = model.predict(X_val_scaled)\n",
    "        val_probas = model.predict_proba(X_val_scaled)\n",
    "\n",
    "        val_trade_df = pd.DataFrame({\n",
    "            'DateTime': val_data['DateTime'], 'pred': val_preds, \n",
    "            'proba': [p[pred] for pred, p in zip(val_preds, val_probas)], \n",
    "            f'Return{horizon}': val_data[f'Return{horizon}']\n",
    "        })\n",
    "        val_returns = val_trade_df.groupby('DateTime').apply(lambda x: calculate_grouped_returns(x, horizon))\n",
    "        horizon_performance[horizon] = (1 + val_returns).prod() - 1\n",
    "        models[horizon] = model\n",
    "\n",
    "    best_horizon = max(horizon_performance, key=horizon_performance.get)\n",
    "    best_model = models[best_horizon]\n",
    "    best_horizons_list.append(best_horizon.replace('_', '') or 'standard')\n",
    "    \n",
    "    X_test = test_data[feature_cols]\n",
    "    y_test = test_data[f'Profit{best_horizon}']\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    test_preds = best_model.predict(X_test_scaled)\n",
    "    test_probas = best_model.predict_proba(X_test_scaled)\n",
    "\n",
    "    test_trade_df = pd.DataFrame({\n",
    "        'DateTime': test_data['DateTime'], 'pred': test_preds,\n",
    "        'proba': [p[pred] for pred, p in zip(test_preds, test_probas)],\n",
    "        f'Return{best_horizon}': test_data[f'Return{best_horizon}']\n",
    "    })\n",
    "    grouped_returns = test_trade_df.groupby('DateTime').apply(lambda x: calculate_grouped_returns(x, best_horizon))\n",
    "    \n",
    "    all_dates.extend(grouped_returns.index)\n",
    "    all_test_returns.extend(grouped_returns)\n",
    "    all_test_preds.extend(test_preds)\n",
    "    all_test_truths.extend(y_test)\n",
    "    all_test_probas.extend(test_probas)\n",
    "    fold_mean_returns.append(grouped_returns.mean())\n",
    "\n",
    "    train_end_date = val_end_date\n",
    "\n",
    "# --- Results ---\n",
    "if all_test_returns:\n",
    "    cumulative_returns = (1 + np.array(all_test_returns)).cumprod() - 1\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(all_dates, cumulative_returns * 100)\n",
    "    plt.title('Cumulative Returns (XGBoost)')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Cumulative Return (%)')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Final Cumulative Compounded Return: {cumulative_returns[-1]*100:.2f}%\")\n",
    "    print(f\"\\nOverall Test Accuracy : {accuracy_score(all_test_truths, all_test_preds)*100:.2f}%\")\n",
    "    print(f\"Overall Precision : {precision_score(all_test_truths, all_test_preds, average='macro', zero_division=0)*100:.2f}%\")\n",
    "    print(f\"Overall Recall : {recall_score(all_test_truths, all_test_preds, average='macro', zero_division=0)*100:.2f}%\")\n",
    "    print(f\"Overall F1 Score : {f1_score(all_test_truths, all_test_preds, average='macro', zero_division=0)*100:.2f}%\")\n",
    "    print(f\"Overall ROC AUC : {roc_auc_score(all_test_truths, np.vstack(all_test_probas), multi_class='ovo', average='macro')*100:.2f}%\")\n",
    "else:\n",
    "    print(\"No valid test periods found.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
